"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[158],{970:(e,n,i)=>{i.d(n,{A:()=>s});const s="data:image/png;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4MDAiIGhlaWdodD0iNDAwIiB2aWV3Qm94PSIwIDAgODAwIDQwMCI+CiAgPHJlY3Qgd2lkdGg9IjgwMCIgaGVpZ2h0PSI0MDAiIGZpbGw9IiNmMGZmZjAiLz4KICA8cmVjdCB4PSI1MCIgeT0iNTAiIHdpZHRoPSI3MDAiIGhlaWdodD0iMzAwIiBmaWxsPSJ3aGl0ZSIgc3Ryb2tlPSIjY2NjIiBzdHJva2Utd2lkdGg9IjIiIHJ4PSIxMCIvPgogIDx0ZXh0IHg9IjQwMCIgeT0iMTAwIiBmb250LWZhbWlseT0iQXJpYWwsIHNhbnMtc2VyaWYiIGZvbnQtc2l6ZT0iMjQiIGZpbGw9IiMyNTYzZWIiIHRleHQtYW5jaG9yPSJtaWRkbGUiIGZvbnQtd2VpZ2h0PSJib2xkIj4KICAgIFZpc2lvbi1MYW5ndWFnZS1BY3Rpb24gU3lzdGVtIEFyY2hpdGVjdHVyZQogIDwvdGV4dD4KICA8dGV4dCB4PSI0MDAiIHk9IjE1MCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE2IiBmaWxsPSIjNGI1NTYzIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj4KICAgIFZpc3VhbCBQZXJjZXB0aW9uIChPYmplY3QgRGV0ZWN0aW9uLCBSZWNvZ25pdGlvbikKICA8L3RleHQ+CiAgPHRleHQgeD0iNDAwIiB5PSIxODAiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNiIgZmlsbD0iIzRiNTU2MyIgdGV4dC1hbmNob3I9Im1pZGRsZSI+CiAgICBOYXR1cmFsIExhbmd1YWdlIFByb2Nlc3NpbmcgKE5MUCwgVW5kZXJzdGFuZGluZykKICA8L3RleHQ+CiAgPHRleHQgeD0iNDAwIiB5PSIyMTAiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNiIgZmlsbD0iIzRiNTU2MyIgdGV4dC1hbmNob3I9Im1pZGRsZSI+CiAgICBBY3Rpb24gUGxhbm5pbmcgYW5kIEV4ZWN1dGlvbgogIDwvdGV4dD4KICA8dGV4dCB4PSI0MDAiIHk9IjI0MCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE2IiBmaWxsPSIjNGI1NTYzIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIj4KICAgIE11bHRpbW9kYWwgSW50ZWdyYXRpb24gYW5kIENvbnRyb2wKICA8L3RleHQ+Cjwvc3ZnPgo="},6683:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var s=i(4848),o=i(8453);const r={title:"Week 8 - Vision Processing",sidebar_position:1,week:8,module:"module-4-vla-systems",learningObjectives:["Understand modern computer vision techniques for robotics","Implement vision-based perception systems for robotic applications","Integrate vision processing with robotic control systems","Apply deep learning models for object detection and recognition","Evaluate vision system performance in real-world scenarios"],prerequisites:[{"Week 1-7 content":"Complete textbook modules"},"Basic understanding of machine learning concepts","Python programming experience","Familiarity with ROS 2 message passing"],description:"Advanced computer vision techniques for robotics applications with deep learning integration"},t="Week 8: Vision Processing",a={id:"modules/module-4-vla-systems/week-8-vision-processing",title:"Week 8 - Vision Processing",description:"Advanced computer vision techniques for robotics applications with deep learning integration",source:"@site/docs/modules/module-4-vla-systems/week-8-vision-processing.md",sourceDirName:"modules/module-4-vla-systems",slug:"/modules/module-4-vla-systems/week-8-vision-processing",permalink:"/Humanoid-AI-Book/docs/modules/module-4-vla-systems/week-8-vision-processing",draft:!1,unlisted:!1,editUrl:"https://github.com/abdullatifjaffrani/Humanoid-AI-Book/tree/main/docs/modules/module-4-vla-systems/week-8-vision-processing.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Week 8 - Vision Processing",sidebar_position:1,week:8,module:"module-4-vla-systems",learningObjectives:["Understand modern computer vision techniques for robotics","Implement vision-based perception systems for robotic applications","Integrate vision processing with robotic control systems","Apply deep learning models for object detection and recognition","Evaluate vision system performance in real-world scenarios"],prerequisites:[{"Week 1-7 content":"Complete textbook modules"},"Basic understanding of machine learning concepts","Python programming experience","Familiarity with ROS 2 message passing"],description:"Advanced computer vision techniques for robotics applications with deep learning integration"},sidebar:"tutorialSidebar",previous:{title:"Module 4: Vision-Language-Action Systems",permalink:"/Humanoid-AI-Book/docs/modules/module-4-vla-systems"},next:{title:"Week 9: Language Integration",permalink:"/Humanoid-AI-Book/docs/modules/module-4-vla-systems/week-9-language-integration"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview",id:"overview",level:2},{value:"Computer Vision Fundamentals for Robotics",id:"computer-vision-fundamentals-for-robotics",level:2},{value:"Image Formation and Camera Models",id:"image-formation-and-camera-models",level:3},{value:"Feature Detection and Matching",id:"feature-detection-and-matching",level:3},{value:"Deep Learning for Vision Processing",id:"deep-learning-for-vision-processing",level:2},{value:"Convolutional Neural Networks (CNNs)",id:"convolutional-neural-networks-cnns",level:3},{value:"Object Detection",id:"object-detection",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"Vision-Based Perception Systems",id:"vision-based-perception-systems",level:2},{value:"2D Vision Systems",id:"2d-vision-systems",level:3},{value:"3D Vision Systems",id:"3d-vision-systems",level:3},{value:"Visual Servoing",id:"visual-servoing",level:2},{value:"Image-Based Visual Servoing (IBVS)",id:"image-based-visual-servoing-ibvs",level:3},{value:"Position-Based Visual Servoing (PBVS)",id:"position-based-visual-servoing-pbvs",level:3},{value:"Multi-Camera Systems",id:"multi-camera-systems",level:2},{value:"Camera Calibration",id:"camera-calibration",level:3},{value:"Sensor Fusion",id:"sensor-fusion",level:3},{value:"Real-Time Vision Processing",id:"real-time-vision-processing",level:2},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Edge Computing for Vision",id:"edge-computing-for-vision",level:3},{value:"Vision in Different Environments",id:"vision-in-different-environments",level:2},{value:"Indoor Environments",id:"indoor-environments",level:3},{value:"Outdoor Environments",id:"outdoor-environments",level:3},{value:"Integration with ROS 2",id:"integration-with-ros-2",level:2},{value:"Vision Message Types",id:"vision-message-types",level:3},{value:"Vision Processing Nodes",id:"vision-processing-nodes",level:3},{value:"Quality Assessment and Evaluation",id:"quality-assessment-and-evaluation",level:2},{value:"Vision System Metrics",id:"vision-system-metrics",level:3},{value:"Robustness Testing",id:"robustness-testing",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Design Principles",id:"design-principles",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Cross-References",id:"cross-references",level:2},{value:"Practice Exercises",id:"practice-exercises",level:2},{value:"Exercise 1: Object Detection Implementation",id:"exercise-1-object-detection-implementation",level:3},{value:"Exercise 2: Camera Calibration and 3D Reconstruction",id:"exercise-2-camera-calibration-and-3d-reconstruction",level:3},{value:"Exercise 3: Visual Servoing System",id:"exercise-3-visual-servoing-system",level:3},{value:"Exercise 4: Multi-Camera Fusion",id:"exercise-4-multi-camera-fusion",level:3},{value:"Discussion Questions",id:"discussion-questions",level:3},{value:"Challenge Exercise",id:"challenge-exercise",level:3},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"week-8-vision-processing",children:"Week 8: Vision Processing"}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand modern computer vision techniques for robotics"}),"\n",(0,s.jsx)(n.li,{children:"Implement vision-based perception systems for robotic applications"}),"\n",(0,s.jsx)(n.li,{children:"Integrate vision processing with robotic control systems"}),"\n",(0,s.jsx)(n.li,{children:"Apply deep learning models for object detection and recognition"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate vision system performance in real-world scenarios"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Vision processing is a critical component of modern robotics systems, enabling robots to perceive and understand their environment. This week explores advanced computer vision techniques specifically designed for robotic applications, with a focus on real-time processing, robustness to environmental variations, and integration with robotic control systems."}),"\n",(0,s.jsx)(n.p,{children:"Modern vision processing in robotics encompasses:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object detection and recognition"}),": Identifying and locating objects in the environment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scene understanding"}),": Interpreting complex scenes for navigation and manipulation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual servoing"}),": Using vision feedback for precise control"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SLAM integration"}),": Combining vision with localization and mapping"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep learning integration"}),": Leveraging neural networks for perception tasks"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"computer-vision-fundamentals-for-robotics",children:"Computer Vision Fundamentals for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"image-formation-and-camera-models",children:"Image Formation and Camera Models"}),"\n",(0,s.jsx)(n.p,{children:"Understanding how images are formed is crucial for robotics applications:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pinhole camera model"}),": The fundamental model describing how 3D points project to 2D image coordinates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intrinsic parameters"}),": Focal length, principal point, and distortion coefficients"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Extrinsic parameters"}),": Camera position and orientation relative to the robot"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\n\ndef project_3d_to_2d(point_3d, camera_matrix, distortion_coeffs):\n    """\n    Project a 3D point to 2D image coordinates using camera parameters.\n\n    Args:\n        point_3d: 3D point in world coordinates [X, Y, Z]\n        camera_matrix: 3x3 camera intrinsic matrix\n        distortion_coeffs: Distortion coefficients [k1, k2, p1, p2, k3]\n\n    Returns:\n        2D point in image coordinates [u, v]\n    """\n    points_3d = np.array([point_3d], dtype=np.float32)\n    rvec = np.array([0, 0, 0], dtype=np.float32)  # Rotation vector (identity)\n    tvec = np.array([0, 0, 0], dtype=np.float32)  # Translation vector (identity)\n\n    points_2d, _ = cv2.projectPoints(points_3d, rvec, tvec, camera_matrix, distortion_coeffs)\n    return points_2d[0][0]\n'})}),"\n",(0,s.jsx)(n.h3,{id:"feature-detection-and-matching",children:"Feature Detection and Matching"}),"\n",(0,s.jsx)(n.p,{children:"Robust feature detection is essential for robotic vision tasks:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SIFT (Scale-Invariant Feature Transform)"}),": Invariant to scale, rotation, and illumination"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SURF (Speeded-Up Robust Features)"}),": Faster alternative to SIFT"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ORB (Oriented FAST and Rotated BRIEF)"}),": Efficient for real-time applications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep features"}),": Learned features from convolutional neural networks"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"deep-learning-for-vision-processing",children:"Deep Learning for Vision Processing"}),"\n",(0,s.jsx)(n.h3,{id:"convolutional-neural-networks-cnns",children:"Convolutional Neural Networks (CNNs)"}),"\n",(0,s.jsx)(n.p,{children:"CNNs have revolutionized computer vision in robotics:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Architecture"}),": Convolutional layers, pooling, and fully connected layers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transfer learning"}),": Using pre-trained models for robotics applications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time inference"}),": Optimizing networks for deployment on robotic platforms"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"object-detection",children:"Object Detection"}),"\n",(0,s.jsx)(n.p,{children:"Modern object detection approaches for robotics:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"YOLO (You Only Look Once)"}),": Real-time detection with single network pass"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"SSD (Single Shot Detector)"}),": Multi-scale detection with anchor boxes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Faster R-CNN"}),": Two-stage detection with region proposal networks"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,s.jsx)(n.p,{children:"Pixel-level understanding of scenes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FCN (Fully Convolutional Networks)"}),": Early approach to semantic segmentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"U-Net"}),": Encoder-decoder architecture with skip connections"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"DeepLab"}),": Atrous convolution for multi-scale context"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"vision-based-perception-systems",children:"Vision-Based Perception Systems"}),"\n",(0,s.jsx)(n.h3,{id:"2d-vision-systems",children:"2D Vision Systems"}),"\n",(0,s.jsx)(n.p,{children:"2D vision systems extract information from single images:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"Vision-Language-Action System Architecture",src:i(970).A+"",width:"800",height:"400"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.em,{children:"Figure 4: Vision-Language-Action system architecture showing the integration of visual perception, language understanding, and robotic action."})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rospy\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass VisionPerception2D:\n    def __init__(self):\n        # ROS setup\n        self.bridge = CvBridge()\n        self.image_sub = rospy.Subscriber("/camera/image_raw", Image, self.image_callback)\n        self.object_pub = rospy.Publisher("/detected_objects", ObjectList, queue_size=10)\n\n        # Object detection parameters\n        self.confidence_threshold = 0.5\n        self.nms_threshold = 0.4\n\n        # Load YOLO model\n        self.net = cv2.dnn.readNetFromDarknet("yolo_config.cfg", "yolo_weights.weights")\n        self.layer_names = self.net.getLayerNames()\n        self.output_layers = [self.layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n\n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n        # Perform object detection\n        height, width, channels = cv_image.shape\n        blob = cv2.dnn.blobFromImage(cv_image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n        self.net.setInput(blob)\n        outputs = self.net.forward(self.output_layers)\n\n        # Process detection results\n        boxes, confidences, class_ids = self.process_detections(outputs, width, height)\n\n        # Apply non-maximum suppression\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.confidence_threshold, self.nms_threshold)\n\n        # Publish results\n        self.publish_detections(boxes, confidences, class_ids, indices)\n\n    def process_detections(self, outputs, width, height):\n        """Process YOLO outputs to extract bounding boxes, confidences, and class IDs."""\n        boxes = []\n        confidences = []\n        class_ids = []\n\n        for output in outputs:\n            for detection in output:\n                scores = detection[5:]\n                class_id = np.argmax(scores)\n                confidence = scores[class_id]\n\n                if confidence > self.confidence_threshold:\n                    # Object detected\n                    center_x = int(detection[0] * width)\n                    center_y = int(detection[1] * height)\n                    w = int(detection[2] * width)\n                    h = int(detection[3] * height)\n\n                    # Rectangle coordinates\n                    x = int(center_x - w / 2)\n                    y = int(center_y - h / 2)\n\n                    boxes.append([x, y, w, h])\n                    confidences.append(float(confidence))\n                    class_ids.append(class_id)\n\n        return boxes, confidences, class_ids\n\n    def publish_detections(self, boxes, confidences, class_ids, indices):\n        """Publish detection results as ROS messages."""\n        if len(indices) > 0:\n            object_list = ObjectList()\n            for i in indices.flatten():\n                obj = Object()\n                obj.x = boxes[i][0]\n                obj.y = boxes[i][1]\n                obj.width = boxes[i][2]\n                obj.height = boxes[i][3]\n                obj.confidence = confidences[i]\n                obj.class_id = class_ids[i]\n                object_list.objects.append(obj)\n\n            self.object_pub.publish(object_list)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3d-vision-systems",children:"3D Vision Systems"}),"\n",(0,s.jsx)(n.p,{children:"3D vision systems provide depth and spatial information:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo vision"}),": Using two cameras to compute depth"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RGB-D cameras"}),": Combining color and depth information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LiDAR integration"}),": Fusing LiDAR and camera data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Structure from Motion (SfM)"}),": Reconstructing 3D from 2D images"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"visual-servoing",children:"Visual Servoing"}),"\n",(0,s.jsx)(n.h3,{id:"image-based-visual-servoing-ibvs",children:"Image-Based Visual Servoing (IBVS)"}),"\n",(0,s.jsx)(n.p,{children:"Controlling the robot based on image features:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature tracking"}),": Maintaining visual features in desired positions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Jacobian computation"}),": Relating image velocity to robot motion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Control laws"}),": Designing stable control systems"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"position-based-visual-servoing-pbvs",children:"Position-Based Visual Servoing (PBVS)"}),"\n",(0,s.jsx)(n.p,{children:"Controlling based on 3D pose estimates:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose estimation"}),": Estimating object or target pose"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task-space control"}),": Controlling in Cartesian space"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sensor fusion"}),": Combining vision with other sensors"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"multi-camera-systems",children:"Multi-Camera Systems"}),"\n",(0,s.jsx)(n.h3,{id:"camera-calibration",children:"Camera Calibration"}),"\n",(0,s.jsx)(n.p,{children:"Proper calibration is essential for multi-camera systems:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\n\ndef calibrate_multi_camera_system(camera_configs):\n    """\n    Calibrate a multi-camera system to determine relative poses.\n\n    Args:\n        camera_configs: List of camera calibration configurations\n\n    Returns:\n        Dictionary containing camera parameters and relative poses\n    """\n    # Initialize camera parameters\n    camera_params = {}\n\n    # For each camera, perform intrinsic calibration\n    for i, config in enumerate(camera_configs):\n        # Load calibration images\n        images = load_calibration_images(config[\'calibration_path\'])\n\n        # Perform intrinsic calibration\n        camera_matrix, distortion_coeffs = calibrate_single_camera(images)\n        camera_params[f\'camera_{i}\'] = {\n            \'intrinsic\': camera_matrix,\n            \'distortion\': distortion_coeffs\n        }\n\n    # Perform extrinsic calibration (relative poses between cameras)\n    extrinsic_params = calibrate_extrinsics(camera_configs, camera_params)\n\n    # Store relative poses\n    for i, j in extrinsic_params:\n        camera_params[f\'camera_{i}_to_camera_{j}\'] = extrinsic_params[(i, j)]\n\n    return camera_params\n\ndef calibrate_single_camera(images):\n    """Calibrate a single camera using checkerboard images."""\n    # Prepare object points\n    objp = np.zeros((6*9, 3), np.float32)\n    objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)\n\n    objpoints = []  # 3d points in real world space\n    imgpoints = []  # 2d points in image plane\n\n    for img in images:\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # Find chessboard corners\n        ret, corners = cv2.findChessboardCorners(gray, (9, 6), None)\n\n        if ret:\n            objpoints.append(objp)\n            imgpoints.append(corners)\n\n    # Perform calibration\n    ret, camera_matrix, distortion_coeffs, rvecs, tvecs = cv2.calibrateCamera(\n        objpoints, imgpoints, gray.shape[::-1], None, None\n    )\n\n    return camera_matrix, distortion_coeffs\n'})}),"\n",(0,s.jsx)(n.h3,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Combining data from multiple sensors:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Kalman filtering"}),": Optimal fusion of multiple sensor measurements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Particle filtering"}),": Non-linear fusion for complex scenarios"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep fusion"}),": Learning-based sensor fusion approaches"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"real-time-vision-processing",children:"Real-Time Vision Processing"}),"\n",(0,s.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.p,{children:"Optimizing vision processing for real-time applications:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware acceleration"}),": Using GPUs, TPUs, or specialized vision chips"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model optimization"}),": Quantization, pruning, and distillation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pipeline optimization"}),": Efficient data processing and memory management"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-threading"}),": Parallel processing of different tasks"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"edge-computing-for-vision",children:"Edge Computing for Vision"}),"\n",(0,s.jsx)(n.p,{children:"Deploying vision systems on robotic platforms:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Jetson platforms"}),": NVIDIA's edge AI computing solutions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Raspberry Pi"}),": Lightweight vision processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedded GPUs"}),": Power-efficient processing for mobile robots"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"FPGA acceleration"}),": Custom hardware for specific vision tasks"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"vision-in-different-environments",children:"Vision in Different Environments"}),"\n",(0,s.jsx)(n.h3,{id:"indoor-environments",children:"Indoor Environments"}),"\n",(0,s.jsx)(n.p,{children:"Challenges and solutions for indoor vision:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lighting variations"}),": Adaptive algorithms for changing illumination"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Texture-poor surfaces"}),": Using geometric features when texture is limited"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Dynamic objects"}),": Handling moving people and objects"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"outdoor-environments",children:"Outdoor Environments"}),"\n",(0,s.jsx)(n.p,{children:"Outdoor vision considerations:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Weather conditions"}),": Robustness to rain, snow, fog"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Illumination changes"}),": Handling shadows and varying sun position"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale variations"}),": Recognizing objects at different distances"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-ros-2",children:"Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"vision-message-types",children:"Vision Message Types"}),"\n",(0,s.jsx)(n.p,{children:"ROS 2 provides standard message types for vision data:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"sensor_msgs/Image"}),": Raw image data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"sensor_msgs/CameraInfo"}),": Camera calibration parameters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"vision_msgs/Detection2DArray"}),": 2D object detections"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"vision_msgs/Detection3DArray"}),": 3D object detections"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"geometry_msgs/PointStamped"}),": 3D points with coordinates"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"vision-processing-nodes",children:"Vision Processing Nodes"}),"\n",(0,s.jsx)(n.p,{children:"Creating modular vision processing systems:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass VisionProcessingNode(Node):\n    def __init__(self):\n        super().__init__('vision_processing_node')\n\n        # Initialize CvBridge\n        self.bridge = CvBridge()\n\n        # Publishers and subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/vision/detections',\n            10\n        )\n\n        # Vision processing components\n        self.camera_info = None\n        self.detector = self.initialize_detector()\n\n        self.get_logger().info('Vision Processing Node initialized')\n\n    def camera_info_callback(self, msg):\n        \"\"\"Store camera calibration information.\"\"\"\n        self.camera_info = msg\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image and publish detections.\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n            # Perform vision processing\n            detections = self.process_image(cv_image)\n\n            # Publish results\n            self.publish_detections(detections, msg.header)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {str(e)}')\n\n    def process_image(self, image):\n        \"\"\"Perform vision processing on the input image.\"\"\"\n        # In a real implementation, this would call deep learning models\n        # For this example, we'll simulate object detection\n\n        # This is a placeholder - in real implementation, use actual detector\n        detections = []\n\n        # Example: Detect a specific object type\n        height, width = image.shape[:2]\n\n        # Simulate detection results\n        for i in range(np.random.randint(1, 4)):  # Random number of detections\n            x = np.random.randint(0, width - 100)\n            y = np.random.randint(0, height - 100)\n            w = np.random.randint(50, 150)\n            h = np.random.randint(50, 150)\n\n            detection = {\n                'x': x,\n                'y': y,\n                'width': w,\n                'height': h,\n                'confidence': np.random.uniform(0.6, 0.95),\n                'class': 'object'\n            }\n            detections.append(detection)\n\n        return detections\n\n    def publish_detections(self, detections, header):\n        \"\"\"Publish detection results as ROS 2 messages.\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            # Create detection message\n            detection_msg = Detection2D()\n            detection_msg.bbox.center.x = detection['x'] + detection['width'] / 2\n            detection_msg.bbox.center.y = detection['y'] + detection['height'] / 2\n            detection_msg.bbox.size_x = detection['width']\n            detection_msg.bbox.size_y = detection['height']\n            detection_msg.results = []  # Add classification results here\n\n            detection_array.detections.append(detection_msg)\n\n        self.detection_pub.publish(detection_array)\n\n    def initialize_detector(self):\n        \"\"\"Initialize the vision detector (placeholder).\"\"\"\n        # In real implementation, load deep learning model here\n        return None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vision_node = VisionProcessingNode()\n\n    try:\n        rclpy.spin(vision_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vision_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"quality-assessment-and-evaluation",children:"Quality Assessment and Evaluation"}),"\n",(0,s.jsx)(n.h3,{id:"vision-system-metrics",children:"Vision System Metrics"}),"\n",(0,s.jsx)(n.p,{children:"Evaluating vision system performance:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Precision and Recall"}),": For object detection tasks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"mAP (mean Average Precision)"}),": Standard metric for detection performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IoU (Intersection over Union)"}),": Spatial accuracy of detections"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Processing time"}),": Real-time performance metrics"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"robustness-testing",children:"Robustness Testing"}),"\n",(0,s.jsx)(n.p,{children:"Testing vision systems under various conditions:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adversarial testing"}),": Testing with challenging scenarios"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Failure mode analysis"}),": Identifying when systems fail"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Uncertainty quantification"}),": Measuring confidence in predictions"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"design-principles",children:"Design Principles"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modularity"}),": Design vision components as reusable modules"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": Handle failure cases gracefully"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficiency"}),": Optimize for real-time performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scalability"}),": Design for different hardware configurations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Maintainability"}),": Write clean, well-documented code"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Validation"}),": Thoroughly validate vision systems before deployment"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback mechanisms"}),": Provide safe behaviors when vision fails"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Redundancy"}),": Use multiple sensors when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Continuously monitor vision system health"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Vision processing is fundamental to robotic perception"}),"\n",(0,s.jsx)(n.li,{children:"Deep learning has transformed robotic vision capabilities"}),"\n",(0,s.jsx)(n.li,{children:"Real-time performance requires careful optimization"}),"\n",(0,s.jsx)(n.li,{children:"Integration with robotic control systems is crucial"}),"\n",(0,s.jsx)(n.li,{children:"Robustness to environmental variations is essential"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"cross-references",children:"Cross-References"}),"\n",(0,s.jsx)(n.p,{children:"This vision processing foundation connects with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/modules/module-1-ros-foundations/",children:"Week 1-3: ROS 2 Foundations"})," - for message passing in vision systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/modules/module-2-gazebo-unity/",children:"Week 4-5: Simulation Basics"})," - where vision sensors are simulated"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/modules/module-3-nvidia-isaac/",children:"Week 6-7: Isaac Platform"})," - where accelerated vision processing applies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/Humanoid-AI-Book/docs/modules/module-4-vla-systems/week-9-language-integration",children:"Week 9: Language Integration"})," - where vision connects with language understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/modules/module-4-vla-systems/",children:"Week 10-13: Humanoid Control and Locomotion"})," - where vision enables robot perception"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practice-exercises",children:"Practice Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-object-detection-implementation",children:"Exercise 1: Object Detection Implementation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement a YOLO-based object detection system using ROS 2 and OpenCV."}),"\n",(0,s.jsx)(n.li,{children:"Integrate the system with a camera feed (real or simulated)."}),"\n",(0,s.jsx)(n.li,{children:"Test the system with various objects and lighting conditions."}),"\n",(0,s.jsx)(n.li,{children:"Measure and report the detection accuracy and processing time."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-camera-calibration-and-3d-reconstruction",children:"Exercise 2: Camera Calibration and 3D Reconstruction"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Calibrate a stereo camera system using OpenCV's calibration tools."}),"\n",(0,s.jsx)(n.li,{children:"Implement a stereo vision system to compute depth maps."}),"\n",(0,s.jsx)(n.li,{children:"Reconstruct 3D points from stereo image pairs."}),"\n",(0,s.jsx)(n.li,{children:"Validate the accuracy of your 3D reconstructions using known objects."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-visual-servoing-system",children:"Exercise 3: Visual Servoing System"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement an image-based visual servoing controller for a simulated robot."}),"\n",(0,s.jsx)(n.li,{children:"Design control laws to move a target object to a desired position in the image."}),"\n",(0,s.jsx)(n.li,{children:"Test the system with different initial conditions and disturbances."}),"\n",(0,s.jsx)(n.li,{children:"Analyze the stability and convergence properties of your controller."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-4-multi-camera-fusion",children:"Exercise 4: Multi-Camera Fusion"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up a multi-camera system with overlapping fields of view."}),"\n",(0,s.jsx)(n.li,{children:"Implement camera calibration to determine relative poses."}),"\n",(0,s.jsx)(n.li,{children:"Create a unified perception system that fuses data from multiple cameras."}),"\n",(0,s.jsx)(n.li,{children:"Compare the performance with single-camera systems in terms of coverage and accuracy."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"discussion-questions",children:"Discussion Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What are the main challenges when deploying deep learning-based vision systems on resource-constrained robotic platforms?"}),"\n",(0,s.jsx)(n.li,{children:"How do environmental factors (lighting, weather, etc.) affect the performance of computer vision systems in robotics?"}),"\n",(0,s.jsx)(n.li,{children:"What are the trade-offs between accuracy and speed in real-time vision processing for robotics?"}),"\n",(0,s.jsx)(n.li,{children:"How can vision systems be made more robust to handle failure cases and unexpected scenarios?"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"challenge-exercise",children:"Challenge Exercise"}),"\n",(0,s.jsx)(n.p,{children:"Design and implement a complete vision-based object manipulation system:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate a 3D vision system (RGB-D or stereo) with a robotic manipulator"}),"\n",(0,s.jsx)(n.li,{children:"Implement object detection and pose estimation for grasp planning"}),"\n",(0,s.jsx)(n.li,{children:"Create a visual servoing system for precise end-effector positioning"}),"\n",(0,s.jsx)(n.li,{children:"Develop a grasping strategy based on visual feedback"}),"\n",(0,s.jsx)(n.li,{children:"Test the complete system in both simulation and, if possible, on real hardware"}),"\n",(0,s.jsx)(n.li,{children:"Document the system architecture, performance metrics, and any challenges encountered"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"/Humanoid-AI-Book/docs/references/vla-bibliography",children:"VLA Bibliography"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>a});var s=i(6540);const o={},r=s.createContext(o);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);